authors,category,content,date,id,img_src,section,tags,title,topics,url
Lucas Matney,Mobile,"YouTube has started rolling out a “Breaking News” section in people’s feeds today across platforms as Alphabet continues to tailor custom content playlists to users logged into Google Accounts, Android Police reports.
For most, YouTube is a place to hop from one video to the next and descend down rabbit holes, but browsing anything like a feed has become less straightforward than other platforms, which makes the breaking news section an interesting addition.

As the video sharing site has grown older, the content has grown more produced with YouTube personalities mounting “celebrity” careers, while commentary-heavy videos grow in popularity over the raw video that is more common on Facebook and Twitter.
For YouTube’s part this has grown to be a very valuable distinction. While Facebook’s has seen its video views increase heavily by way of quick-and-dirty videos, YouTube seems to be somewhere where people invest major time browsing, even if there seems to be just as much noise. In June, YouTube CEO Susan Wojcicki announced that the site had 1.5 billion watching an hour of video each on mobile alone.
Breaking news may initially seem to be a bit of an unusual direction for the site, but as Google works on courting publishers through special programs with AMP, it may be interesting to see how YouTube treats partnerships with 24-hour broadcast news publishers who already put a lot of content on the site.
Today, my “breaking news” tab had a lot of news on Bannon’s exit from the White House. Most of the stuff was an hour or two old, so it’s not BREAKING quite as much as other locations but it offers a chance to see stories evolve on YouTube in a much more packaged way. For now  it seems that YouTube is mostly turning to traditional networks as the sources. Going with established media might be a more straight-forward start but it will be interesting to see how much it adjusts to people’s viewing habits and whether more bombastic YouTube personalities find their way into the tab sharing their thoughts on the day’s latest news and begging you to like and subscribe for more.",2017-08-18,1528781,https://tctechcrunch2011.files.wordpress.com/2017/04/youtube-ios.jpg?w=738,mobile/,,YouTube starts delivering ‘breaking news’ on its homepage across platforms,"google,youtube",https://techcrunch.com/2017/08/18/youtube-starts-delivering-breaking-news-on-its-homepage-across-platforms/
Josh Constine,Apps,"Should your parents be able to read your Facebook messages if you die? Facebook explained why it won’t let them in a post in its Hard Questions series today about social networking after death.
Facebook admits it doesn’t have all the answers, but it has come up with some decent solutions to some issues with what it calls Memorialized Profiles and a “Legacy Contact.” When you pass away, once Facebook is informed, the word “Remembering” appears above your name on your profile and no one else can sign in to your account.

The Legacy Contact is a friend you select in your Manage Account Settings while you’re still alive, though they’re not informed until your profile is memorialized. They can pin a post atop your profile, change your profile pic, respond to friend requests or have your account removed. But Facebook explains they can’t log into your account, change or delete old posts, remove friends or read your messages.
Similarly, Facebook won’t allow parents or anyone else to read your messages after you die. That’s because “In a private conversation between two people, we assume that both people intended the messages to remain private,” writes Monika Bickert, Facebook’s Director of Global Policy Management. The Electronic Communications Privacy Act and Stored Communications Act may also prohibit it from sharing private communications even with parental consent.
Facebook also tries to minimize the emotional impact of losing a loved one by no longer sending birthday reminders about writing on their wall. But there are still plenty of opportunities for hurt feelings. Facebook’s On This Day feature and others can surface old content from when that person was still alive, creating an unexpected experience of having to think about their death.

The company has built features to enhance empathy with its users, allowing them to avoid unnecessarily seeing their exes on the app after a break-up. But it’s tough to know what will be a sweet nostalgic reminder and what will be a heart-wrenching spiral into the past.
What’s important is that Facebook is at least thinking and talking about these issues. Now at 2 billion users, Facebook has become a ubiquitous utility that impacts every phase of our lives. “There’s a deep sense of responsibility in every part of the company,” says Facebook CPO Chris Cox. “We’re getting to the scale where we have to get much better about understanding how the product has been used.”",2017-08-18,1528526,https://tctechcrunch2011.files.wordpress.com/2017/08/what-happens-on-facebook-when-you-die.png?w=738,mobile/,,How Facebook prioritizes privacy when you die,facebook,https://techcrunch.com/2017/08/18/facebook-after-death/
Darrell Etherington,Mobile,"Google is using a astronomical event to debut its newest Android operating system update: Android O is set to arrive on August 21, with a livestreamed unveiling event timed for 2:40 PM ET in NYC – which is roughly when the maximum solar eclipse is set to occur for New York.
Android O will get a full reveal at that time, which seems like kind of a weird time to do it since a lot of people will be watching the NASA eclipse livestream that Google is also promoting, or staring at the sky (with the caveat, hopefully, that they have procured proper glasses for safe viewing).
Google says that Android O will have some “super (sweet) new powers,” most of which we know all about thanks to pre-release builds and the Android O teaser Google provided at its annual I/O developer event this past May. WE know, for instance, that the notification panel has been changed significantly, and there’s new optimization software to improve battery life on all devices.
Also, it looks like it’s all but confirmed that the official consumer name of Android O will be ‘Oreo,’ as has long been speculated. Google always names its Android releases after sweet treats, and O seems like a no-brainer for the iconic cookie. Also, prolific leaker Evan Blass also posted this on Friday following the announcement of the reveal date and event:",2017-08-18,1528522,https://tctechcrunch2011.files.wordpress.com/2017/08/oreo-android-o.png?w=738,mobile/,,Google to livestream Android O launch event timed for total solar eclipse,"android,google",https://techcrunch.com/2017/08/18/google-to-livestream-android-o-launch-event-timed-for-total-solar-eclipse/
Romain Dillet,Apps,"Instagram inception, here we come. If somebody sends you a photo or video in a private conversation on Instagram, the app will now let you play around with the original photo so that you can reply in a creative way and keep the context of the conversation.
If you receive a photo or video, there’s now a reply button in the conversation thread. If you tap on this button, the original photo is instantly turned into a sticker in the top right corner. You can leave it there and reply with some context
But you can also move it around, tilt it and draw around it. If you’re replying to a video, it looks like Instagram only keeps a screenshot of the first frame.
If you tap on it, the original photo fills the top half of the screen. You can then take a selfie for the bottom half of the photo. It reminds me of Frontback, a photo-sharing app that lets you take a photo of what you have in front of you, and a photo of your reaction with a selfie. Instagram works the same way as it features both the context and your reaction.
The comparison stops there as you can still apply filters, draw on your photo, add stickers, write text and use all those creative tools together. Your reaction doesn’t have to be a photo either. You can record a video, a boomerang, use a selfie filter and more.
More importantly, this new reply mode isn’t limited to direct messages. If you’re watching a story, you can either send a text reply by tapping on the text field at the bottom of the screen, or you can send a photo/video reply by tapping on the camera icon at the bottom left of the screen. And that makes a lot of sense as you want to know the context when somebody is replying to part of your story.
",2017-08-18,1528366,https://tctechcrunch2011.files.wordpress.com/2017/08/reply-2-en.png?w=738,mobile/,,Instagram adds a new creative way to reply to a photo or story,instagram,https://techcrunch.com/2017/08/18/instagram-adds-a-new-creative-way-to-reply-to-a-photo-or-story/
Sarah Buhr,Diversity,"TouchID has always been a legal grey area when it came to cops and border patrol asking you to fork over your phone. But, with the new update in iOS 11, that no longer seems to be a problem.
The new feature is being referred to in some circles as a “cop button” because it allows the iPhone owner to set up a provision in the update allowing you to choose whether to keep TouchID on or not. This allows travelers and people who tend to get harassed by police more often — or just anyone concerned with privacy and security — to set their phone up using a long, complex password, thus locking out cops and anyone who doesn’t know the passphrase.
Currently, police can force you to use your fingerprint to unlock the phone, but they can’t force you to use your password — something that has been proven by law to be protected.
As Twitter user @alt_kia pointed out, to unlock the phone you press the power button rapidly five times and it will open a second screen, while also forcing anyone with the phone to use the longer passphrase to unlock it.
This process is especially useful in an emergency situation where you need to unlock the phone and call 911. As noted in Engadget, the option to call for help will pop up after you push the power button five times.
Of course, the process isn’t easy for those wishing to use it for privacy reasons, and you might get fatigued constantly unlocking it this way. However, it’s a good way to keep what’s on your phone to yourself.",2017-08-17,1528288,https://tctechcrunch2011.files.wordpress.com/2017/08/10575772326_af7e69b7dd_k-1.jpg?w=738,mobile/,"cops,ios",An iOS 11 feature could let you quickly disable TouchID and keep cops out,apple,https://techcrunch.com/2017/08/17/an-ios-11-feature-could-let-you-quickly-disable-touchid-and-keep-cops-out/
Josh Constine,Apps,"Ever gotten tricked into clicking a fake play button on Facebook that opens a link instead of starting a video? I did, repeatedly, and wrote a story in 2014 titled “Yo Facebook, Ban Links With Fake Video Play Buttons”.
Now Facebook is doing just that. Today it started downranking the News Feed presence of links that display a fake play button in the preview image, as well as videos that are actually just a static image uploaded as a video file. Publishers who use these scammy tactics will see a major decrease in the distribution of these stories. Facebook won’t completely delete these posts, though, unless they violate its other policies.
Here are two examples of fake play buttons that spammers used to steal your clicks:

Facebook has prohibited the use of fake play buttons in advertisements under its policy against depicting non-existent functionality for a few years, News Feed Product Manager Greg Marra tells me. But the scourage has remained in the News Feed.
“We’ve heard from people who are frustrated by fake play buttons” Marra says, hence today’s update. “Spammers are using these tactics to trick people into clicking links to low quality web pages.Facebook tells me its now training its machine vision artificial intelligence to classify and detect fake play buttons in preview images.
“While the prevalence is statistically low, the frustration expressed by people who use Facebook who encounter these deceptive practices is high” a spokesperson tells me.
Facebook says that if publishers want to denote there’s a video behind a link, they should indicate that through Open Graph meta tags. They could also use words like “Watch” or “Video” in the headline or description.
 
Facebook has had a similar problem with publishers looping pre-recorded videos and calling them live, or just putting up a computer graphic countdown and calling it Live. TechCrunch called on Facebook to ban these shenanigans back in January, and it cracked down on them in May.
There’s also been the issue of publishers putting fake Instant Articles “Lightning Bolt” icons on the preview images of links to non-Instant Articles on the standard web. That’s because people are more likely to click Instant Articles since they load faster.
Meanwhile, Facebook’s emphasis on video in News Feed has inspired the new menace of publishers uploading a static image as a video to get more eyeballs. These static image videos will be downranked too. Facebook is using a “motion scoring” system that detects movement inside a video to classify and demote these clips.

Today’s changes come as part of a massive, multi-pronged atack on clickbait. Facebook now downranks headlines that are misleading or withhold information in many languages, shows fewer links overshared by spammers, works with outside fact checkers to demote false news, promotes iand now shows Related Articles with different angles to make people suspicious of exaggerated clickbait.
With each of these updates, Facebook chips away at the clickbait problem, leaving more room in the News Feed for legitimate content. Getting burned by trying to watch a video which is just endless minutes of the same image erodes trust in the News Feed, making people less likely to watch videos in the future.
By excising these annoying experiences, users may be willing to browse longer, view more videos from friends and publishers, and watch lucrative video ads that fund Facebook’s soaring profits.",2017-08-17,1528083,https://tctechcrunch2011.files.wordpress.com/2017/08/facebook-fake-video-play-buttons.png?w=738,mobile/,,Facebook downranks video clickbait and fake play buttons,facebook,https://techcrunch.com/2017/08/17/facebook-fake-play-buttons/
Darrell Etherington,Gadgets,"The Essential Phone has arrived, a bit later than originally announced. The first smartphone from the new company founded by Android creator Andy Rubin is now available to order, via Essential’s own site, Best Buy, and Sprint. The phone is still listed as a pre-order in all three spots, with shipping information to be conveyed later, but this is the closest people have been able to get to actually holding the device in their hands up until now.
Essential also began sending out notifications to early customers who had pre-registered to purchase the device. 9to5Google reported Wednesday that pre-registered users were receiving emails telling them to supply their payment information, and that once completed, their devices would begin shipping within seven days.
The Essential Phone features a nearly edge-to-edge display, and a standard 128GB of storage on board. It supports external accessories, including a 360-degree camera that Essential revealed at the same time as its phone. The Phone supports all major U.S. carriers, but it’s being sold exclusively via Sprint with a $260 discount at retail. Unlocked, it’s $699 from Essential.com, with a limited time offer to also get the 360 camera in a $749 bundle.
Essential’s smartphone is designed to be minimal in its approach to branding, and to other stuff that phone makers typically do to mess up the smartphone experience, like preloading apps and content, or recreating their own, substandard versions of stock Android apps. It’s also a premium device in terms of materials and design, and Essential is promising two years of Android OS updates and three years of security updates, too.
It sounds like the first Essential Phone customers will still have to wait a week or so to actually receive their devices, but this is a big milestone for Rubin’s company. Another premium smartphone maker entering the fray is also bound to make things a bit more interesting in the market, which has seemed to settle with Apple and Samsung ensconced firmly at the top.",2017-08-17,1528004,https://tctechcrunch2011.files.wordpress.com/2017/05/essential-phone1.jpg?w=738,mobile/,essential,"Essential Phone now available to order, ships soon to pre-sale customers",,https://techcrunch.com/2017/08/17/essential-phone-now-available-to-order-ships-soon-to-pre-sale-customers/
Natasha Lomas,Europe,"Rebooting the venerable Nokia smartphone brand has not been a rush job for HMD Global, the Foxconn-backed company set up for the purpose of licensing the Nokia name to try to revive the brand’s fortunes on smartphones.
But after starting with basic and mid-tier smartphones, it’s finally outted a flagship Android handset, called the Nokia 8, which it will be hoping can put some dents in Samsung’s high end. And/or pull consumers away from Huawei’s flagships handsets — or indeed the swathe of Chinese OEMs surging up the smartphone market share ranks.
With the Nokia 8, HMD is putting its flagship focus on content creators wanting to livestream video for their social feeds.
Competition in the Android OEM space has been fierce for years and there’s no signs of any slack appearing so HDM faces a steep challenge to make any kind of dent here. But at least it now has an iron in the fire. As analyst CCS Insight notes, the handset will be “hugely important in getting Nokia-branded smartphones back on the mobile phone map”.
Specs wise, the Nokia 8 runs the latest version of Android (Nougat 7.1.1) — which HMD is touting as a “pure Android experience”, akin to Google’s Pixel handsets. (There’s a not-so-gentle irony there, given Nokia’s history in smartphones. But clearly HMD is going full in on Android.)
On the hardware front, there’s a top end Qualcomm Snapdragon 835 processor, plus 4GB of RAM and 64GB of internal memory (expandable thanks to a MicroSD card slot). While the 5.3 inch Quad HD resolution display puts it on the verge of phablet territory — and squarely within the current smartphone screen size sweet spot.
Also on board: dual rear cameras, both 13MP (one color, one B&W), and a 13MP front facing lens — all with f/2.0; using Zeiss optics; and with support for 4K video.
The flagship camera feature — and really phone feature too — is the ability to livestream video from both front and back cameras simultaneously.
HMD is trying to coin a hashtaggable word to describe this: “bothie” (as opposed to a selfie)…
This split screen camera feature can also be used for photos — so they’ve basically reinvented Frontback. Well done.
“Content creators can natively broadcast their unique #Bothie stories to social media through the Dual-Sight functionality located within the camera app. Fans can also enjoy unlimited photo [<16MB in size] and video uploads to Google Photos,” HMD writes.
This could prove a sticky feature for social media lovers — perhaps especially the dual video option, which lets people share twin perspective video direct to Facebook and YouTube via the camera app.
Or it could prove a passing fad, like Frontback. Time will tell. CCS Insight describes it as an “interesting approach” but also cautions on whether consumers will take to it.
Commenting on the feature in a statement, HMD’s Juho Sarvikas, chief product officer, said: “We know that fans are creating and sharing live content more than ever before, with millions of photos and videos shared every minute on social media. People are inspired by the content they consume and are looking for new ways to create their own. It’s these people who have inspired us.”
Elsewhere on the device, there’s a spatial surround sound recording tech that uses three microphones and is apparently drawing on Nokia’s Ozo 360 camera division, plus USB type C charging port; a 3.5mm headphone jack; and a non-removable 3090 mAh battery.
The handset, which is clad in an aluminium unibody casing and has a fingerprint reader on the front for device unlocking and authentication, is described as splashproof rather than waterproof.
Global RRP for the Nokia 8 is €599, with a rollout due to start in September. The handset comes in a choice of four colors: Polished Blue, Polished Copper, Tempered Blue and Steel.
This post was updated with a correction: the display is Quad HD (2560 x 1440), not ultra HD as originally stated",2017-08-17,1527946,https://tctechcrunch2011.files.wordpress.com/2017/08/small-nokia-8-family-1.jpg?w=738,mobile/,"smartphones,nokia",Android newbie HMD’s Nokia 8 flagship lets you livestream ‘frontbacks’,,https://techcrunch.com/2017/08/17/android-newbie-hmds-nokia-8-flagship-lets-you-livestream-frontbacks/
Connie Loizos,eCommerce,"Two years ago, Ember launched a crowdfunding campaign on Indiegogo to build a mug that keeps hot drinks hot and iced drinks cool. Contributors gave the company nearly $362,000. Fast-forward and the five-year-old startup has now raised just north of $24 million altogether, including a $13 million Series C round that it quietly closed last week.
The individual investors supporting the company are undoubtedly encouraged by the progress it has been making since showing off its early product to the public. For one thing, Starbucks began selling the mugs in its stores across most of the U.S. and online for $149.95 back in November. The Westlake Village, Ca.-startup also sells its mugs on Amazon, where 186 customers have now assigned them a collective 3.5 stars out of five. (The biggest knock against the product by Ember’s customers: that its battery, which takes up to two hours to charge, doesn’t last terribly long. The company’s purported plans to develop a car charger might help on this front. )
Altogether, the company, whose mugs also can be purchased at its site, says it has sold more than 20,000 mugs so far. It has also inspired at least one new player, a company in Salt Lake City that recently turned to Kickstarter to raise funds for its own heated smart mug, called The Jül.
Ember isn’t breaking out who joined its most recent financing, though it has said in the past that its investors include StubHub CEO Scott Cutler, eBay chief product officer RJ Pitman, singers Demi Lovato and Drew Taggart of The Chainsmokers, and Robert Brunner, chief designer of Beats by Dre and the former head of design at Apple.
According to L.A. Biz, the company plans to use some of that fresh capital to expand its product set, including building a temperature-controlled baby bottle, chilled water bottles and dinner plates that can be made to stay warm.",2017-08-29,1531723,https://tctechcrunch2011.files.wordpress.com/2016/11/ember_mug_lady.jpg?w=738,gadgets/,ember,"Ember just raised $13 million for its popular, temperature-controlled mugs",indiegogo,https://techcrunch.com/2017/08/29/ember-just-raised-13-million-for-its-popular-temperature-controlled-mugs/
Devin Coldewey,Gadgets,"One of the many considerations we will have should we decide at last to colonize another planet is where we’ll live. Should we bring inflatable habitats? Should we ship girders and metal sheets? Or should we, as explored in a recent NASA challenge, 3D-print the structures right there on the planet in question? Two universities’ early efforts to do so earned them a combined $400,000 at a competition held last week.
It’s not the first nor last of these challenges, but the culmination of phase two of the three-phase 3D-Printed Habitat Challenge. The idea is to figure out what materials, designs and other choices might factor into creating a strong, simple structure.
Beams, cylinders and domes were what the teams had to create, with materials that were at least 70 percent “indigenous material” — so something that could be found locally. After all, you don’t want to have to ship a bunch of concrete mix to Phobos.
The winning teams both chose “powdery Crushed Basaltic Igneous” as their indigenous base; the team from Branch Technology augmented this with recycled plastic, while Penn State attempted (with partial success) to create a cement-like material that incorporated water.

Besides the materials chosen and basic dimensions of the items required, the teams were relatively unfettered. “The rules were formulated so that the teams could have freedom to formulate a solution with as much flexibility as possible,” explained a NASA representative who was part of the event. “Both solutions have their own advantages and disadvantages, and these diverse solutions can help NASA formulate that best solution.”
Giant 3D printers were used to create the items in question, after which they were graded on various measures and then crushed — sorry, “compressed to failure.” The Branch team’s dome handled an impressive 1,694 kilograms, or 373 lbs. Penn State’s collapsed under just below half of that — but the NASA rep pointed out that the cement-like mixture would have been far stronger had it been able to cure for another week or so.
This is both a strength and a weakness, exactly the kind of data NASA hoped to glean from the competition. As in the Cube Quest Challenge, the agency proudly advertises that it relies on the fertile imaginations of students and outsiders to augment its own expertise.
The Branch team took home $250,000 for its efforts, and Penn State was awarded $150,000. It’s not clear whether this also puts them in pole position for phase three of the challenge, which my otherwise informative NASA rep declined to enlarge upon. All we know is that it will focus on “fabrication of complete habitats,” which sounds about two orders of magnitude more difficult than phase two. You can keep up with the latest at the challenge’s website.",2017-08-29,1532682,https://tctechcrunch2011.files.wordpress.com/2017/08/nasahab-2.jpg?w=738,gadgets/,nasa,3D-printed space habitats earn $400K in prizes at NASA competition,3d-printing,https://techcrunch.com/2017/08/29/3d-printed-space-habitats-earn-400k-in-prizes-at-nasa-competition/
Sarah Perez,Gadgets,"Amazon this morning announced a new feature for its Echo devices called “multi-room music,” which allows owners with more than one Echo to control where their music is played, or even sync it across a group of Echo devices using voice commands. For example, you’ll be able to tell Alexa to play your music in the living room, play music upstairs or downstairs, or even just play your music everywhere.
Beyond just saying “play my music,” followed by a location, you can also ask Alexa to play your favorite artist, or use other music-related commands, as you would normally.
The feature, at launch, works across some – but not all – of the music services supported by Echo. Naturally, it supports Amazon’s own music service (which also just slashed its student pricing today), as well as TuneIn, iHeartRadio and Pandora. The company says that support for Spotify and Sirius XM is coming soon, but didn’t provide an exact timeframe.
Echo devices, Echo Dot and Echo Show smart speaker systems will support multi-room music in the U.S., U.K., and Germany, starting today.
The ability to synchronize your music across your home could make Echo devices a poor-man’s alternative to pricier home audio and speaker systems. While Amazon’s speakers aren’t necessarily known today for having the best sound quality – it’s Alexa that’s really selling them – rumor has it that Amazon is working on a high-end speaker to compete with Apple’s forthcoming HomePod.
Adding multi-room music support ahead of an updated, better quality Echo device makes sense as gives time for Amazon to get its software working, as well as put tools into the hands of other device makers that will allow their own speaker systems to work with Alexa in this way, too. That could potentially impact demand for Apple’s device.
On this front, Amazon also today introduced the Alexa Voice Service (AVS) multi-room music SDK, which will allow device makers to integrate with the new music feature. Once enabled on devices, consumers can stream their music across Echo and other AVS devices at the same time. For instance, you could play your music on a couple of Echos and a set of standalone speakers at the same time.
The SDK is becoming available early next year, but a sign-up form is available now.
Related to this, new connected speaker APIs are also rolling out that will allow device makers of connected audio systems to control their music playback using Alexa. This one is particularly clever because it makes Echo a companion device and not a competitor to other popular audio systems.
Amazon says it’s already working with Sonos, Bose, Sound United, and Samsung to integrate this technology into their devices. That means you could use an Echo – perhaps an Echo Dot, which is too small and underpowered to deliver great audio – to ask Alexa to play your music on your Sonos system instead.
“Alexa set the standard for voice in smart homes, so working with Amazon to bring voice control to Sonos for the first time was an obvious choice,” said Antoine Leblond, VP of Software at Sonos, in a statement.
“This has been a close collaboration from the beginning as we’ve worked together to combine the magic of Alexa with the seamless multi-room audio capabilities that Sonos pioneered. We’re proud of the work we’ve done together as Amazon’s first multi-room partner – all you’ll need is an Alexa-enabled device and playing music out loud on Sonos will be as easy as saying ‘Alexa, play music in the living room,'” Leblond added.
Sonos, it was also revealed this week by way of an FCC filing, is getting into the voice-controlled speaker game itself, with plans to sell a speaker that would have voice control, far-field microphones and supports “multiple voice platforms,” the filing said. It plans to announce a new device on October 4th, at an event in NYC.
The connected speaker APIs are launching into a developer preview today.",2017-08-29,1532908,https://tctechcrunch2011.files.wordpress.com/2017/08/gettyimages-801805876.jpg?w=738,gadgets/,"music-streaming,music,smart-speakers",You can now play your music across multiple Echo devices,amazon,https://techcrunch.com/2017/08/29/you-can-now-play-your-music-across-multiple-echo-devices/
Darrell Etherington,Gadgets,"Sonos is almost certainly going to throw its hat in the smart speaker ring on October 4, when it’s hosting an event in NYC. We know about its forthcoming voice-enabled speaker because such a device was filed with the FCC, and the filing describes a speaker with voice control and far field microphones capable of picking up voice commands in a big room and filtering out background noise.
The invitation TechCrunch received features an open mouth, and in the lower left corner, an image that was present in the FCC filing which looks like a controller graphic complete with a microphone icon and play/pause controls. Taken together, it’s basically a lock that Sonos will reveal its own voice-enabled hardware on October 4.

That will mean Sonos will have its product announced (and potentially available) ahead of the actual ship date for Apple’s own HomePod speaker, which Apple unveiled in June but will be shipping only in late Q4 this year.
Sonos’ speaker will reportedly feature support between multiple voice assistants, according to the FCC filing, which would make it by far the most flexible of the current range. Offering access to a user’s platform of choice could be a game-changing feature for a smart speaker, especially when paired with the sound, build and multi-room streaming quality Sonos is already known for.
One more clear clue: The Verge received a separate invite, which, when paired with ours, reveals a clearer look at that interface graphic, which was shows a microphone icon atop that new controller graphic.
",2017-08-29,1532863,https://tctechcrunch2011.files.wordpress.com/2017/08/sonos.jpg?w=738,gadgets/,sonos,Sonos sends out invites for smart speaker reveal on October 4,,https://techcrunch.com/2017/08/29/sonos-sends-out-invites-for-smart-speaker-reveal-on-october-4/
Devin Coldewey,Artificial Intelligence,"Uber’s engineering blog has just posted an interesting piece on the company’s web-based tool for exploring and visualizing data from self-driving car research. It’s a smart look at an impressive platform, and definitely has nothing to do with a long piece published last week lauding a similar platform in use by one of Uber’s most serious rivals, Waymo.
Okay, maybe it has a little to do with that. The piece, over at The Atlantic, is quite interesting, but seemed rather to suggest that Waymo is unique in its approach to improving its autonomous cars’ AI. In fact, it’s likely that every company working on this stuff has a pretty similar approach, at least if they’re keeping pace with the state of the art.
The cool secret technique that in fact all the companies in question know about is the possibility of using and learning from data that’s been hoarded over a million miles of test driving. Once you’ve driven that far, you have so much data that you can mix and match it in a virtual environment and let the AI navigate it just as if it were real. The computer doesn’t know the difference! Meanwhile you can tweak the data, watch for unusual events or compare multiple models.
The Uber post just focuses on visualization of this data, and with details on its tools, which are wisely web-based, leading to easy collaboration and quick turnaround on new features. These days web apps can access the GPUs, communicate in real time and so on — no need for a local client any more for many things. It makes for cool GIFs.
What the post doesn’t really get into, but is pretty much a foregone conclusion given the sophistication of the tools they’re showing off, is how to further multiply the data’s value by essentially making up the environment out of whole cloth.
Take for example the problem of dealing with a major event like a parade or protest. Would you let your naive self-driving car run free during a marathon just so it has a chance to learn how the runners act? Of course not.
What you can do is open up your excellent map of Boston, shut down several main thoroughfares, add lots and lots of pedestrians and erratic drivers to the virtual world and then set your AI driver agents to work getting around. You’ll see when it breaks down, how it reacts to situations it’s never seen in real life and so on. It’s like a thought experiment that generates usable data and improves the AI.
So maybe the timing is just a coincidence, but it seems like this post, while cool on its own, is Uber’s way of saying “Hey, we’re doing this too. Look!” Because at this point, if you’re an autonomous car developer and you’re not using simulations with all kinds of variations, you’re going to have a bad time.",2017-08-28,1532704,https://tctechcrunch2011.files.wordpress.com/2017/04/uber-vs-waymo.png?w=738,gadgets/,"autonomous-vehicles,self-driving-cars",Uber shows off its autonomous driving program’s snazzy visualization tools,uber,https://techcrunch.com/2017/08/28/uber-shows-off-its-autonomous-driving-programs-snazzy-visualization-tools/
Devin Coldewey,Gadgets,"We’ve mostly moved past the point where our Internet of Things devices leak private information to anyone watching via unsecured connections, but that doesn’t mean you can stop being afraid. Never, ever stop being afraid. To top up your paranoia reserves, a new study finds that internet providers can, if they so choose, monitor all kinds of things from your smart home’s traitorous metadata.
The paper, from a team at Princeton’s computer science school led by grad student Noah Apthorpe, gets straight to the point: “we demonstrate that an ISP or other network observer can infer privacy sensitive in-home activities by analyzing internet traffic from smart homes containing commercially available IoT devices even when the devices use encryption.”
It’s a pretty straightforward attack: the IoT devices often identify themselves voluntarily, usually by connecting to specific domains or URLs. Even if they didn’t, there are simple ways of profiling them based on observation and some known data. The researchers demonstrated this by showing that various devices show distinct patterns of data transmission:
Once they’re identified, the ISP (in this case played by the researchers) can simply watch for increases in traffic. What those changes in traffic mean are either self-evident or perfectly able to be inferred with a little analysis.
By watching a sleep tracker, the ISP can see when the user gets in bed and wakes up, perhaps even how well they sleep, whether they get up in the middle of the night and so on.
By watching various smart switches, the ISP can see when certain devices are in use: the TV, the space heater, the light in the basement, the garage door.
By watching the IP security camera traffic, the ISP can see when the camera detects motion, when the user is tuned in to watch their home from afar or when they check archived footage.
And if a handful of academics can do it, you better believe a major ISP could — though of course they’ll tell you they won’t. Doesn’t matter, they can collect this stuff and sell it without telling you, since Congress zapped the FCC’s privacy protections. The researchers note this in the paper, in fact.
But don’t worry, there’s actually a pretty good solution! The team found that by transmitting the IoT data through a central hub (e.g. a router with a little custom software), they could effectively camouflage it by transmitting a trickle of junk data at all times. This traffic shaping, as it’s called, doesn’t prevent the devices from working (many of them worked surprisingly well with artificially slowed connections), but it does make it hard for an attacker to tell signal from noise.
They suggest a constant stream of around 40 KB/s should be more than enough, though that adds up over time to over a hundred gigabytes — not something everyone can afford, depending on data caps. But that system could easily be improved or made more amenable to people with limited bandwidth.
The full paper is quite readable and is available here.",2017-08-28,1532634,https://tctechcrunch2011.files.wordpress.com/2016/05/shutterstock_351658193.png?w=738,gadgets/,,Internet providers could easily snoop on your smart home,internet-of-things,https://techcrunch.com/2017/08/28/study-tracks-what-smart-home-activity-can-be-seen-by-internet-providers/
Devin Coldewey,Gadgets,"If you like to keep on top of the latest display tech, you’re probably familiar with Ray Soneira’s exhaustive testing of the screens on the newest smartphones and other devices. His latest target is the Galaxy Note 8, and it turns out Samsung’s excellent phablet sports the best screen they’ve ever seen on a phone.
It shouldn’t come as that great of a surprise — OLED screens are the future, of course, and although early models were far from competitive, steady improvements have led to the tech leapfrogging traditional LCDs.
But don’t take my word for it. Look at that beautiful signature up top. And more importantly, Soneira’s article concludes:
Of particular interest were the phone’s multiple calibrated display modes, all of which have as good or better color accuracy than the best screens out there — but have subtle or not-so-subtle differences in how they render an image. There’s also a user-adjustable white balance setting, which photographers and anyone sensitive to color temperature will appreciate.
The full rundown of new and improved features is worth reading, so take a few minutes to check out the state of the art.
An interesting thing to think about given the surpassing excellence of the Note 8 display is whether others — particularly Apple — can challenge them. If the iPhone is indeed going to go to OLED soon, it likely wouldn’t be able to claim anything but playing catch-up, though Apple has been spinning its catch-up moves pretty well over the last few years. But although its IPS LCDs are counted among the best in the world, it’s going to be a stretch for them to say they’re superior to the competition.
Right now, however, the main problem is there just aren’t enough of the high-quality OLED units to supply everyone who wants them, let alone Apple. So it’s a bit of a moot point for the present. 2018, however, may be a very interesting one for the display market.",2017-08-28,1532442,https://tctechcrunch2011.files.wordpress.com/2017/08/samsung-galaxy-note-8-21.jpg?w=738,gadgets/,galaxy-note-8,Tests put Galaxy Note 8 at the top of the smartphone display heap,samsung,https://techcrunch.com/2017/08/28/tests-put-galaxy-note-8-at-the-top-of-the-smartphone-display-heap/
Matt Burns,Gadgets,"DJI wants security researchers to turn their attention to its software and drones and will pay for discovered bugs or exploits. Called The DJI Threat Identification Reward Program, the program aims to create a formal line of communication between researchers and hackers to the drone maker. The company will pay between $100 and $30,000 for qualifying bugs, “depending on the potential impact of the threat.”
This program comes after a high-profile ban by the U.S. Army grounded the company’s drones after unspecified “cyber vulnerabilities” were discovered. Immediately following that ban, DJI added an offline mode that prevents any data from being sent to or received from the internet. But clearly there’s a need for a deeper inspection of the company’s wares.
DJI says this program was created to identify threats to users’ private data, videos, and logs. But it doesn’t stop there. DJI is also looking at issues that could result in flight safety concerns, such as DJI’s geofencing restrictions, flight altitude limits and power warnings.
“We want to engage with the research community and respond to their reasonable concerns with a common goal of cooperation and improvement,” DJI Director of Technical Standards Walter Stockwell said in a released statement. “We value input from researchers into our products who believe in our mission to enable customers to use DJI products that are stable, reliable and trustworthy.”
Discovered bugs can be emailed to bugbounty@dji.com.
Drones need to be safe if they’re becoming a mass-market hit that reaches deep into the consumer and commercial world. This is a long-overdue step that will hopefully result in drones that are harder to hit with malicious attacks.",2017-08-28,1532333,https://tctechcrunch2011.files.wordpress.com/2017/08/dji-drone.jpg?w=738,gadgets/,"emerging-technologies,aviation,aircraft,dji",DJI launches bug bounty program for its software and drones,,https://techcrunch.com/2017/08/28/dji-launches-bug-bounty-program-for-its-software-and-drones/
Darrell Etherington,Gadgets,"The Insta360 brand has quickly become known for consumer 360 photography products, and the new Insta360 One is its latest device designed for mobile immersive imaging. The new Insta360 One offers 4K video resolution, and 24 MP stills, as well as the ability to work in a completely standalone mode, remotely with your phone as a Bluetooth controller, and plugged in directly to your iPhone using your smartphone’s display as the viewfinder.
This latest Insta360 camera comes with a number of improvements beyond image quality, however, including a new ability to select an angle and export a 1080p non-surround video clip from whatever perspective you choose, and upping its image stabilization capabilities to allow for much smoother video, even when handheld, and to create a ‘bullet time’ video effect in slow motion using a unique capture method – circling the camera overhead quickly using either a selfie stick or a piece of string to create a super cool slow motion 360 video.
Said string is included attached to a standard tripod threaded mount that screws into the bottom camera, letting you get started with this cool feature right away. The string is also edited out of the image automatically, as are any selfie sticks you use with the camera, which makes for a much cleaner resulting image or video.
The bullet time effect is very cool, but it’s important to follow the instructions to get it right. The resulting images really do look like something pulled from The Matrix, with you as the shooter in the center and the work around you nearly frozen in time as the image zooms around. It’s a new twist on the selfie, but it also requires a lot of available space to work well – don’t expect to be doing this much indoors, or with a lot of other people in your immediate area.
It’s a very neat feature, but that’s probably going to be more broadly useful about the Insta360 One over time is its general flexibility. The camera has a built in tripod mount on its main body, and comes with a protective case that doubles as a stand. Its remote capabilities mean you no longer have to be that close to it to use it, and it can still plug directly in your iPhone when you want to see exactly what you’re capturing, too.

There’s an Android version coming soon, too, so don’t feel left out if that’s your platform of choice. And if you’re looking for shooting options, there are a number of different ones including RAW support for images and Log format for video, which means you can do a lot when editing the resulting files. The camera also supports time-lapse capture, as well as full manual camera settings, and its output options also offer a range of different choices.
This is easily the best consumer 360 camera I’ve tried out so far, because of its range of options, and the results of its photos and video, which are high quality for a device that doesn’t cost thousands of dollars. Insta360’s stabilization, stitching and companion app are all terrific, and make working and sharing with the content you capture super easy.
The Insta360 One also supports Facebook Live streaming, which is great if you want to show off a special event or concert, and its 1080p video export feature is convenient for capturing everything and then setting a frame for sharing later. You can even track a specific subject using a feature called “SmartTrack,” to tag a face and have a resulting 1080p non-surround video output as well.

I also got to try it out with a Bluetooth remote and selfie stick that Insta360 plans to sell as accessories. There’s a forthcoming waterproof housing, too, that will turn this into an all-condition and even underwater camera, up to a depth of 30 meters. That’s definitely going to make it a constant vacation companion for me.
The world of 360 consumer cameras is admittedly getting a bit crowded, especially considering how many people are probably actually consuming 360 content. But the Insta360 One seems like a clear favorite now, even with its $299 price tag. There’s a lot of value in that price, and this is definitely my top recommendation right now if you’re looking to get into surround video and photos.
Orders are up now for the Insta360 One, and it’ll start shipping on September 5, with accessories available later. Is the box, you get a micro SD card, a micro USB cable for charging, the case and the string attachment for bullet time shots.",2017-08-28,1532118,https://tctechcrunch2011.files.wordpress.com/2017/08/insta360.jpg?w=738,gadgets/,insta360,The Insta360 One is a consumer 360 camera with tons of flexibility,,https://techcrunch.com/2017/08/28/the-insta360-one-is-a-consumer-360-camera-with-tons-of-flexibility/
Matt Burns,Gadgets,"It looks like Sonos is nearing the release of a speaker that has a built-in voice control assistant. According to an FCC filing found by Dave Zatz, the company is seeking to get approval from the U.S. Government to sell a speaker that sports “integrated voice control functionality with far field microphones” and “multiple voice platforms.”
It’s impossible to say if Sonos is building in Alexa, but the company has been making moves as of late to play friendly with Amazon’s voice platform and in the past expressed plans to build the voice control platform into its speakers.
Back in January the new CEO sent a memo to his staff challenging them to boldly innovate, and to partner with global leaders, which he named as Amazon, Google and “(likely) Apple.” Since then the company started rolling out services that links Sonos speakers with Alexa devices, allowing for voice control. This function is a start and great for current speakers, but the speaker shown in this FCC filing would take the integration to the logical conclusion.
According to the FCC filing, the unreleased Sonos will have far-field microphones much like Amazon Echo devices. The speaker would also support multiple voice platforms and music services. Most of this report is redacted so the details mostly stop there.
For Sonos the best move would be to let users pick between Amazon, Google, and maybe, eventually, Apple’s home voice control assistant. That could be what is mentioned here, though since the speaker is still in testing, it’s unclear what function will eventually launch with the speaker.
The company could reveal more details at the upcoming CEDIA tradeshow in early September, which features home control, and audio and video companies.",2017-08-28,1532209,https://tctechcrunch2011.files.wordpress.com/2013/10/sonos.jpg?w=738,gadgets/,"federal-communications-commission,speaker,technology,microphone",Sonos is testing a speaker with a mic and voice control,,https://techcrunch.com/2017/08/28/sonos-is-testing-a-speaker-with-a-mic-and-voice-control/
Darrell Etherington,Gadgets,"The GameBoy Pocket remains one of the best-designed pieces of mobile gaming kit ever in my opinion, which is why I was intrigued by the BittBoy, a new third-party portable console that clearly owes a lot of inspiration to the GameBoy. The tiny handheld packs a library of “300” games built-in, controls that will feel familiar to any Nintendo fan, a battery you can recharge via microUSB and a 2.2-inch IPS display.
First, let’s get to the highlights: This thing is super small. It’s only about 2.5 by 4 inches, and it’ll easily slip into just about any pocket. The battery also appears to last quite a while, even though it’s only 500 mAh; I’ve played for quite a few hours on a single charge without depleting it yet.
That screen is also pretty good, especially when compared to the screens of actual retro consoles. There are some issues with colors being rendered weird, but that’s the emulation software, not the display itself, as those color oddities carry over to TV output, too.
Speaking of TV output, that’s another strength – it ships with a simple mini stereo to RCA AV out, which can plug into any TV with one of those yellow round video inputs. It works, though the cable is somewhat short and the image flickers a bit if there’s much cable play, but it came in handy when spending time in a hotel and looking for something to do to pass some time.
The BittBoy’s biggest strength is its ability to go anywhere while taking up virtually no space in your kit, or adding any weight to your baggage. It’s so light you might actually forget you’re carrying it, as I have on occasion before emptying my pockets. The buttons and case, while not the most luxurious feeling in the world, also seem like they can take a fair amount of abuse before breaking down.
BittBoy’s weaknesses are probably in its software library: It offers “300-in-1” games, but you probably aren’t going to want to play most of them. Plus, they’re emulated NES titles, so you have to be okay with that legal gray area.

There’s plenty enough there to sustain the attention of retro gaming fans, however – especially at $39.99, the retail price of BittBoy. But if you’re hoping to expand the library with your own software, look elsewhere, since there’s a microSD-looking slot on top but it’s not functional.
Bottom line: This is a fun stocking stuffer or add-on gift for the avid retro gamer in your life, and something that you can basically stick in a regularly used backpack as a fun surprise you can re-discover on long commutes or while traveling.",2017-08-27,1531969,https://tctechcrunch2011.files.wordpress.com/2017/08/bittboy-147a1612.jpg?w=738,gadgets/,,BittBoy is a retro pocket console that does a lot for very little,,https://techcrunch.com/2017/08/27/bittboy-is-a-retro-pocket-console-that-does-a-lot-for-very-little/
Darrell Etherington,Gadgets,"Keep your dogs close but your kitties closer, is I think the expression. Maybe. Whatever it is, pets are important to people. And keeping a close watch on them even when you’re not located in the same place has become increasingly possible thanks to the broad proliferation of tech like Wi-Fi connected cameras. Petcube Bites is one of those cameras, but it’s also a treat dispenser, which lets you fire nibblets for your pet back at home from wherever you happen to be, provided you have an active internet connection.
Fair warning: This is a much larger unit than your average Nest or Logitech smart home camera. It’s roughly the size of a big encyclopedia, or a couple of Mac minis standing on their thin edge and stacked. The size is functional, however, since it houses a decent-sized reservoir for your stock of treats, as well as an HD camera, a Wi-Fi radio, and a mechanism for actually firing off those treats, with a user-selectable range of up to around 6 feet.

Petcube Bites ships with a selection of all-natural treats included, so you can get started using it right away. My dog isn’t exactly discerning when it comes to what she eats (she ate half a pizza box the other day), but I can say that she happily scarfed these down.
Setup of the Pectcube Bites is easy if you’ve ever installed any kind of Wi-Fi camera or connected home device before – and it should pose no problem even if you haven’t. It basically involves getting the unit connected to your Wi-Fi network using an app you download for your iOS or Android smartphone.
Once connected, the Bites will show up in the app, and you can connect to it to view a live feed from the camera, and to do additional stuff including initiating a two-way voice conversation if you feel like ‘talking’ to your pet. You can also trigger the dispensing of a treat, of course, and this is where the Bites departs from a more rudimentary two-way connected home camera.

Using the app, you simply flick up on the smartphone screen to launch a treat. Again, you can set the distance, and after. few seconds the treat will drop into the tray and then get kicked out across the room. I found this worked as designed pretty consistently, although once in a while two treats were dispensed instead of one, which is not really a huge issue, and probably would be described as a feature, not a bug, by my dog. You can also adjust portion sizes, and even set a dispensing schedule if you want them released at pre-set times.
My dog pretty quickly learned that the whirring of the little motor within the unit mean that a treat was on its way, so she would appear in frame just before the treat was launched almost every time I sent one her way. It’s amazing how fun it is to play that way with your pet when you have to be away from home.
There are some nice features the Petcube Bites offers beyond just the pet treat dispensing. It has motion detection, with notifications that push to your device (which you can turn off if you like). Video quality is also not bad at all, compared to comparatively priced dedicated home security cams. If you’ve got a pet and you’re already considering a Wi-Fi camera, this might be a better option in terms of value for money.

Other advantages nice things about the Petcube Bites include the multiple installation options – you can either use the rubberized base to just sit it on a flat surface, or use the pre-drilled holes on the back to simply slot it over mounting screws in a wall. Again, it’s large, so having options in terms of how to use it is a nice benefit for anyone concerned with how well it will (or won’t) fit into their home decor.
Petcube Bites is available in three color ways for $229.99 right now from Petcube direct. Additional features, including historical cloud video storage, are available as part of additional paid monthly plans.",2017-08-26,1531916,https://tctechcrunch2011.files.wordpress.com/2017/08/petcube-bites-1.jpg?w=738,gadgets/,petcube,Petcube Bites is a capable canine companion for when you’re not home,,https://techcrunch.com/2017/08/26/petcube-bites-is-a-capable-canine-companion-for-when-youre-not-home/
Devin Coldewey,Gadgets,"The Defense Department’s research arm has officially kicked off its effort to create a modular computing framework, with pieces pulled from a mix-‘n-match set of “chiplets.” Producing something this weird will take a village, the agency suggested — in fact, “an enormous village rife with innovators.” DARPA did always have a way with words.
The program, first announced last year, is called Common Heterogeneous Integration and Intellectual Property Reuse Strategies, which they abbreviate to CHIPS. It’s been reaching out to universities, military-industrial contractors, and of course the semiconductor and chip biz to explore the possibility, and this week was the “proposers’ day,” when the agency and interested parties share details and expectations.
Basically the idea is to reduce certain functions to standard chiplet size and form factor, within reason, and create a system by which those chiplets can be organized into larger boards. Need a board that’s heavy on image processing and storage for a satellite or recon drone? Put a bunch of those pieces together. Want something more focused on low-latency signal processing and integrating input from multiple sensors? Forget the image stuff and snap in some other parts.
A slide deck presented at the event this week (PDF) has lots more details, though since the project is still in early stages it’s all still pretty speculative.
It’s unclear what size or form the chiplets would take — that’s up to the creators, the innovators in that enormous village, to decide. It could be macro-level swapping in, like popping in extra RAM or a PCI card. Or it could be baked in at the manufacturing level but still more flexible than existing custom chip systems.
Ideally, though, the resulting electronics would be smaller, more versatile, and cheaper to make and replace than current solutions — which wouldn’t be hard in some cases, with some military systems dating decades into the past.
DARPA was keen to emphasize that it doesn’t want anything remade from scratch, merely retooling things to create a more flexible infrastructure. The old paradigm of the do-it-all PC isn’t the best any more in many cases. That may, however, mean establishing new interfaces or standards.
	
Dan Green, the program manager, had more winged words to utter for CHIPS:
“Now we are moving beyond pretty pictures and mere words, and we are rolling up our sleeves to do the hard work it will take to change the way we think about, design, and build our microelectronic systems.”",2017-08-26,1531816,https://tctechcrunch2011.files.wordpress.com/2017/08/chips_program.jpg?w=738,gadgets/,"modular,darpa",DARPA project aims to make modular computers out of ‘chiplets’,,https://techcrunch.com/2017/08/26/darpa-project-aims-to-make-modular-computers-out-of-chiplets/
Darrell Etherington,Gadgets,"The Essential Phone started arriving in the hands of media and reviewers just last week, and as of today it’s now also shipping out to its first customers. Essential announced the news via its Twitter account, and told early buyers to keep an eye out for tracking info sent out via email.
Last week, pre-registered customers started getting emails asking them to provide final payment and shipping information to prepare for shipping to start soon. Essential then held a few press events to demonstrate the new device in-person.
The Andy Rubin-founded smartphone maker is selling its device directly through its website unlocked for $699, and via carrier partner Sprint in the U.S. and Telus in Canada. It opened pre-orders through each of these sellers just last week, and through Best Buy in the U.S. While Sprint and Telus are the phone’s only official carrier partners at launch, it will work with all major carriers in both the U.S. and Canada unlocked.
Essential had first announced the device would ship within 30 days in late May, but after missing that target it has remained relatively quiet on firm shipping timelines. The news that the first shipments have started to go out will likely be welcomed by the brand’s early buyers. If you happen to be one of those lucky customers receiving tracking info, drop us a note in the comments or via email to let us know.
Here’s a bonus for reading: A direct size comparison of the Essential Phone (right) versus the Samsung Galaxy Note 8 (left), just for funsies.",2017-08-25,1531716,https://tctechcrunch2011.files.wordpress.com/2017/08/essential-phone-3.jpg?w=738,gadgets/,"andy-rubin,essential",Essential Phone begins shipping to customers,,https://techcrunch.com/2017/08/25/essential-phone-begins-shipping-to-customers/
Darrell Etherington,Automotive,"CarPlay and Android Auto can only really be described as what you’d call a “slow burn.” They both debuted quite a few years ago, but getting access to them via first-party infotainment systems didn’t happen with the pace early adopters might be accustomed to. Luckily, a variety of third-party aftermarket in-car audio and infotainment decks offer an option for those who don’t want to buy a new car or wait for their automaker of choice to catch up with the times.
Pioneer’s new AVH-2330NEX, which the company officially revealed in May this year and started shipping just recently, offers both Android Auto and Apple CarPlay in a single package, which is very convenient if you happen to spend time on both platforms. It also offers a CD/DVD slot, accessible via the sliding front faceplate, and a 7-inch resistive touchscreen with 800×480 screen resolution. Physical buttons are included below the display, as is Bluetooth for hands-free and stereo streaming.
The whole thing comes in a double-DIN package that will work with a wide range of vehicles, with or without trim kits depending on your car. It also can plug into factory steering wheel controls, as well as first-party and aftermarket backup cameras. The AVH-2330NEX also ships with its own microphone, which is great because, in my experience, it offered far better voice recognition for things like Google Assistant and Siri, as well as for calls, when compared with factory-installed microphones shipping on vehicles.

At $500, the AVH-2330NEX (and the AVH-2300NEX,which is the same minus the inclusion of HD radio support, a second camera input and remote control) is far cheaper than either buying a new car, or even in many cases paying for the infotainment upgrade package that will get you access to CarPlay and Android Auto in stock vehicles. It’s also a big boost for even factory sound systems, as it greatly improved the audio quality of both Bluetooth and wired music and podcast playback in my 2013 Nissan Rogue thanks to its built-in DAC.
But the real benefit here is access to your mobile OS of choice, right in your dash, front and center where you want it when driving. Plus, thanks to a standard USB adapter, you can switch between both using various cables, including micro-USB, USB-C and Lightning depending on your device’s requirements.

I found that both CarPlay and Android Auto ran well on the Pioneer 2330NEX, and I encountered no issues with either during many hours of extensive testing. Text was large and legible on the 7-inch display, as were icons and other interface elements. Animations and transitions were smooth, too, and I never encountered any kind of lag, which can happen on underpowered infotainment systems running these in-car mobile software systems.
The other thing I was a bit wary of going into testing the AVH-2330NEX was the lack of a capacitive touchscreen. It uses a resistive unit instead, which historically hasn’t been as good for touch interfaces on electronic devices: That was one of the iPhone’s chief strengths, in fact — it brought capacitive touchscreens to mass-market mobile devices, offering a much better overall user experience.
Surprisingly, the resistive screen made very little difference versus capacitive in-car units I’ve tried in the past. I never encountered errors with missed touch input, and the screen felt responsive enough. I suspect that’s a combination of the tech having advanced over the years and an infotainment deck not requiring the same high level of responsiveness as a smartphone in order to feel like it’s working properly.

Either way, the resistive display works perfectly fine and keeps the cost down. It does, however, mean that the AVH-2330NEX has a plastic-covered display that does not do well in direct sunlight. In fact, that’s my single major complaint with the unit: The screen doesn’t look terrific in bright light. It’s still perfectly usable, but it’s not ideal.
Aside from CarPlay and Android Auto, the Pioneer AVH-2330NEX offers a lot of great features, including support for a backup camera. That also worked flawlessly in my testing, kicking in whenever I put my car in reverse. The steering wheel controls are also great (though that’ll depend a bit on your car make and model). One small caveat there, too: It’s unlikely you’ll be able to map your call button to Siri or Android Assistant, but the Pioneer deck itself has a dedicated hardware button with a mic label that lets you activate those at any time with relative ease and a minimum of distraction.
The Pioneer AVH-2330NEX offers what few products can: A new lease on life for older vehicles, especially for tech-focused users who want to experience the latest in infotainment convenience features but don’t want to have to buy a whole new car to do it.",2017-08-25,1527981,https://tctechcrunch2011.files.wordpress.com/2017/08/pioneer-avh-2300nex-feature.jpg?w=738,gadgets/,pioneer,Pioneer’s AVH-2330NEX gives you both Android Auto and CarPlay — without a new car price tag,,https://techcrunch.com/2017/08/25/pioneers-avh-2330nex-gives-you-both-android-auto-and-carplay-without-a-new-car-price-tag/
Darrell Etherington,Enterprise,"Uber is introducing a major revamp of their Uber for Business platform today, the first significant update they’ve done since the enterprise tool’s introduction. The new Uber for Business incorporates a lot of user feedback to provide easy setting of rules to ensure travel policy is followed, as well as group-based access levels and custom program creation.
Previously, a lot of the heavy lifting on employee Uber use policies had to be done manually: HR would basically tell employees what they were and weren’t allowed to do in terms of Uber usage, and hope that employees would stick to the letter of the policies in place. Now, however, companies using Uber for Business can set those rules ahead of time to streamline the process of expensing rides, and ensure greater compliance.
Managers can easily create programs that limit things like the type of car used, the total amount riders can expense, what time of day Uber can be used and even geographic limits on where it can be expensed. These rules can all be combined in custom programs, including things like First and Last Mile programs for commuters, recruiting and client travel programs, employee perks, and general travel transport expense programs.
These programs will display once a rider selects their business account, based on eligibility when they go to book a ride through the rider app as they would normally. Businesses can assign users to groups depending on their role, and can use those groups to define eligibility for specific programs.
If a rider’s travel could fall out of policy, say for a trip that goes beyond the available credit in an employee perk program, they’ll be prompted to add their own payment method so that they can complete the journey while still staying fair of their company’s rules. It’s a system that takes into account both user convenience and the concerns of expense compliance teams in a way that benefits both in the end.
Uber has also redesigned the manager-facing backed for Uber for Business, making it easier to use. That includes incorporating Uber Central directly into the interface, so that approved managers can manage and assign rides for clients, customers, freelancer, recruits and others not directly covered by the existing policies in place.
Overall, this is a great redesign with new features that make it very easy to implement and use for businesses of all sizes, and managers of all levels of technical expertise. Uber’s clearly looking at business as a prime market for future growth, and this looks like a no-brainer for expense departments looking to ease the workload.",2017-08-15,1527118,https://tctechcrunch2011.files.wordpress.com/2017/08/context-image.png?w=738,enterprise/,uber-for-business,Uber debuts new Uber for Business with custom travel programs and rules,uber,https://techcrunch.com/2017/08/15/uber-debuts-new-uber-for-business-with-custom-travel-programs-and-rules/
Frederic Lardinois,Cloud,"Microsoft today announced that it has acquired Cycle Computing, a twelve-year-old Connecticut-based company that focuses on helping enterprises orchestrate high-performance computing jobs, large data workloads and other “big computing” jobs in the cloud. The financial details of the deal were not disclosed.
While Microsoft plans to use the company’s expertise to improve its Azure service for these kind of high-end workloads, Cycle Computing’s flagship CycleCloud service always supported a wide range of cloud and on-premises platforms, including AWS and the Google Cloud Platform. Microsoft notes that the Cycle Computing tech will help it improve its support for Linux-based high-performance computing workloads.
Current Cycle Computing customers include the likes of Novartis, Pacific Life, MetLife and other major manufacturing, insurance, biotech and media companies. Cycle Computing, which was bootstrapped and never raised a “real” funding round, says that its service will manage about a billion core-hours of compute this year and that it has grown 2.7x every 12 months.
“We’ve already seen explosive growth on Azure in the areas of artificial intelligence, the Internet of Things and deep learning,” Jason Zander, Microsoft’s corporate VP of Azure, writes in today’s announcement. “As customers continue to look for faster, more efficient ways to run their workloads, Cycle Computing’s depth and expertise around massively scalable applications make them a great fit to join our Microsoft team.”
Cycle Computing co-founder and CEO Jason Stowe  writes that his company will continue to support its existing customers, though it’s unclear if this means that Microsoft will also continue to develop support for competing platforms. We have reached out to Microsoft for clarification and will update this post once we hear more.
Update: Here’s Microsoft’s statement, which seems to imply that Microsoft will continue to support current clients but won’t put any new development work into support for the AWS and GCP platforms: “We will continue to support Cycle Computing clients using AWS and/or Google Cloud. Future Microsoft versions released will be Azure focused. We are committed to providing customers a seamless migration experience to Azure if and when they choose to migrate.”
 ",2017-08-15,1527131,https://tctechcrunch2011.files.wordpress.com/2017/08/gettyimages-452289950.jpg?w=738,enterprise/,"acquisition,cycle-computing",Microsoft acquires Cycle Computing,microsoft,https://techcrunch.com/2017/08/15/microsoft-acquires-cycle-computing/
Frederic Lardinois,Cloud,"Amazon’s AWS cloud computing service hosted its annual NY Summit today and it used the event to launch a new service: Amazon Macie. The idea behind Macie is to use machine learning to help businesses protect their sensitive data in the cloud. For now, you can use Macie to protect personally identifiable information and intellectual property in the Amazon S3 storage service, with support for other AWS data stores coming later this year (likely at the re:Invent conference in November).

The company says the fully managed service uses machine learning to monitor how data is accessed and to look for any anomalies. The service then alerts users of any activity that looks suspicious so they can find the root cause of any data leaks (whether those are malicious or not). To do all of this, the service continuously monitors new data that comes into S3. It then uses machine learning to understand regular access patterns and the data in the storage bucket.
The service also automatically detects certain data types like full names, addresses, credit card numbers, IP addresses, driver license IDs (U.S. only), social security numbers and birth dates, but it also can automatically detect different content types (email, SEC forms, data logs, database backups, source code, etc.).
All of this data then flows into a central dashboard that highlights high-risk files and other information about how users and other applications are accessing data.

As with all AWS services, pricing is complicated, but mostly based on the number of events and data the service processes every month. Because a lot of costs are bound to the initial classification of the data, the first month of usage is also likely the most expensive.
For now, Macie is only available in AWS’s U.S. East (Northern Virginia) and U.S. West (Oregon) regions, though this footprint will likely expand over time.
It’s worth noting that Amazon also announced that Glue, the company’s service for preparing and loading data into its various database and storage services, is now available to all customers. In addition, the company used today’s event to launch a new migration hub for enterprises that want to migrate some of their workloads to the cloud, as well as updates to the Elastic File System (now with encryption at rest), AWS Config and AWS CloudHSM for better key management.",2017-08-14,1526736,https://tctechcrunch2011.files.wordpress.com/2014/06/aws-robot.jpg?w=738,enterprise/,"security,cloud,aws",Amazon Macie helps businesses protect their sensitive data in the cloud,amazon,https://techcrunch.com/2017/08/14/amazon-macie-helps-businesses-protect-their-sensitive-data-in-the-cloud/
John Mannes,Artificial Intelligence,"U.S. Secretary of Defense James Mattis is concluding his tech tour of the West Coast today with a visit to Google’s Mountain View campus. Mattis spent time at Amazon and DiuX, the Defense Innovation Unit Experimental, earlier in the week. His key takeaway from all the socializing with tech leaders is that the Department of Defense needs to embrace technology coming out of the private sector if it hopes to stay ahead of the technological curve.
In a briefing before Mattis’ media address at DiuX, Sean Singleton, director of business development for the unit, explained that commercial R&D is outpacing federal R&D at a rate of nearly three to one. And companies like Amazon and Alphabet spend more on R&D than traditional military contractors like Lockheed Martin and Raytheon.
To stop the military from falling behind the times, the Defense Innovation Unit Experimental works with startups to accelerate commercial innovation for national defense. DiuX provides non-dilutive capital to companies willing to work with the government to solve specific problems. The group’s work is similar to In-Q-Tel, a VC firm that brings the CIA closer together with tech companies.
The unit is focused on autonomy, AI, human systems/life sciences, information technology/cyber and space. But of those five investment areas, AI remains a key enabling technology for all of them. And Secretary Mattis was the first to admit that he sees many of the greatest advances in AI happening in the private sector.
“The bottom line is we will get better at integrating the advances in AI that are being taken here in the Valley into the U.S. military,” Secretary Mattis told reporters at the DiuX briefing.
Underscoring this point, Singleton told the story of DiuX managing director Raj Shah going into the desert with Eric Schmidt, executive chairman of Alphabet. The two observed archaic processes for planning out tanker routes for jets with Schmidt insisting that the problem was fixable with technology, it just needed attention.
To date, DiuX has invested in Quid, Saildrone, Orbital Insight, Shield AI and more. These companies don’t solely produce products and services for the U.S. Government, but they promise to dedicate resources to address public sector priority problems.
For example, Shield AI drones can scan a house in a military context to identify threats. The drones and accompanying software also can serve the private sector, monitoring oil and gas fields and surveying construction sites.
DiuX currently has four offices across the United States, in Cambridge, Mass., Mountain View, Calif., Washington, D.C., and Austin. The 48-person team is expected to grow to 75 by the end of 2018, assuming Secretary Mattis is genuine about his commitment to build bridges between the military and private sector tech companies.",2017-08-11,1526279,https://tctechcrunch2011.files.wordpress.com/2017/08/20806600_10211482534117960_74209337_o.jpg?w=738,enterprise/,"raytheon,lockheed-martin,defense",The US Government must work with tech companies if it wants to remain competitive in AI,eric-schmidt,https://techcrunch.com/2017/08/11/the-us-government-must-work-with-tech-companies-if-it-wants-to-remain-competitive-in-ai/
John Mannes,Artificial Intelligence,"There are a million and one services for voice transcription on the market. But even with just one job to do, I’ve never seen a service that can handle the long tail of vocabulary used in the real world. This is particularly challenging if you’re a startup trying to sell your service to enterprises that rely on accurate transcription for their operations.
Jon Goldsmith, co-founder of Tetra, a voice transcription startup, understands this challenge — in fact, he is even willing to admit that he hasn’t 100 percent cracked the problem. But Goldsmith believes the answer lies in deep learning, and he’s setting out to prove it with a $1.5 million seed round led by Amplify Partners, with participation from Y Combinator and a number of angels.
I dropped by the Tetra office to check out what Goldsmith, his co-founder Nik Liolios and one other engineer had created. Goldsmith gave me a call using his smartphone with the Tetra app installed. As he and the deep learning models running in the background listened, I threw out a barrage of challenges for the transcription service.
Speaking at varying speeds, throwing out numbers, startup names and other tough words did stump Tetra to some degree — but to be fair, there is no AI that I haven’t broken. Given how easy Tetra is to use, I could see it being used as a backup reference or for record keeping — turn it on, forget about it and use it to search through notes later.
In cases where 99 or 100 percent accuracy is required, Tetra offers human transcription for a fee and a 24-hour wait. This actually helps both customers and Tetra in the sense that accurate transcriptions can feed back as training data to improve future performance.
Goldsmith told me he is finding traction selling to investors making frequent diligence calls. These customers want Tetra to create a permanent record of conversations with industry experts. Other, more traditional, enterprise use cases exist as well, like within sales.
This seems to be working out fairly well for the company. And things remain fairly lean with the three-person Tetra team working out of a residential apartment dually zoned for commercial. On the engineering side, a lot of the underlying infrastructure is being powered by off-the-shelf APIs.
This is actually a good thing, because it means Tetra isn’t wasting time building things that already exist on the market and instead is focusing on collecting a massive transcriptions data set that will only continue to improve the quality of the service moving forward.
The team’s approach is heavily dependent on being able to optimize which parts of conversations are sent to which cloud API. For example, some NLP service providers are better at understanding speech relating to movies, music and media, while others are better at numbers, etc.
The $1.5 million in seed financing is going to be used to scale up the engineering team and improve machine learning pipelines. Tetra includes search functionality so users can quickly find specific sentences within traditionally unsearchable voice recordings. I could see this becoming more proactive in the future — flagging names and dates automatically, for example.",2017-08-10,1525802,https://tctechcrunch2011.files.wordpress.com/2017/08/img_0369.jpg?w=738,enterprise/,"linguistics,economy,writing,transcription",Tetra raises a $1.5M seed round to bring deep learning to voice transcription,,https://techcrunch.com/2017/08/10/tetra-raises-a-1-5m-seed-round-to-bring-deep-learning-to-voice-transcription/
Frederic Lardinois,Enterprise,"Interest in blockchains is at an all-time high, but there are still plenty of technical issues to solve, especially for enterprises that want to adopt this technology for smart contracts and other use cases. For them, issues like throughput, latency, governance and confidentiality are still major stumbling blocks for using blockchains. With its new Coco Framework, Microsoft wants to solve these issues and make blockchains more suitable for the enterprise.
In an interview earlier this week, Microsoft’s CTO for Azure (and occasional novelist) Mark Russinovich told me the company is seeing a lot of interest in blockchain technology among its users. They like the general idea of a distributed ledger, but a system that can only handle a handful of transactions a second doesn’t work for them — what they want is a technology that can handle a thousand or more transactions per second.
The Coco Framework solves these fundamental issues with blockchains by introducing a trusted execution environment (TEE). The basic idea here is that you have a trusted box on which you can trust to put your blockchain code. That trust is established through tools like Intel’s Software Guard Extensions or Windows’ Virtual Secure Mode — and because it’s an open framework, it can also support other TEEs as they become available. With these TEEs you can then build a network of trusted enclaves that all agree on the ledger and Coco code they are running (it’s a concept Intel and others have also experimented with in the past).
Once you have these trusted enclaves, all the other pieces fall into place. Because you can trust the updates to the ledger, you don’t need to perform any proof of work, which greatly increases the transaction speed. In a typical ledger, that would take seconds or even minutes. But with the enclaves, that’s not an issue, and Microsoft says Coco and Ethereum can handle up to 1,600 transactions per second in its prototype setup. Thanks to this, the blockchain network — when combined with a protocol like Paxos for ensuring consistency — becomes a usable database.
Enterprises also want to ensure that one of their suppliers can’t see the orders you placed with another supplier. That’s a hard problem to solve when your ledger is public. Coco, however, adds a confidentiality layer on top of a ledger like Ethereum (or any other ledger, because the framework is ledger-agnostic). Enabling this only took some minor addition to the Ethereum protocol in Microsoft’s prototype.
Another feature Coco enables is governance. To explain this, Russinovich used the example of a bank consortium that wants to use a blockchain network. Who gets to add another bank to this network? The Coco governance system allows the members of the consortium to set up rules for voting on decisions like this.
What’s important here is that the Coco Framework will be compatible with any ledger protocol and can run virtually anywhere — in the cloud or on premises, and on any operating system and hypervisor that supports a compatible trusted environment. R3 Corda, the Intel-incubated Hyperledger Sawtooth and J.P. Morgan Quorum will integrate their distributed ledgers with Coco.
“We are thrilled to work with Microsoft to bring blockchain to the enterprise,” said Rick Echevarria, vice president, Software and Services Group and General Manager, Platforms Security Division at Intel, in today’s announcement. “Our mutual customers are excited by the potential of blockchain. Intel is committed to accelerating the value of blockchains powered by Azure on Intel hardware, by improving the scalability, privacy and security of the solutions based on our technologies.”
Don’t get too excited yet, though. It’ll be early 2018 before Microsoft will open source the code for Coco. Russinovich tells me the team is still hardening the code and getting it ready for open sourcing. The company is making a technical whitepaper and demonstrations available today, though.",2017-08-10,1525256,https://tctechcrunch2011.files.wordpress.com/2016/12/blockchain.png?w=738,enterprise/,"distributed-ledgers,blockchain",Microsoft wants to make blockchain networks enterprise-ready with its new Coco Framework,"microsoft-azure,microsoft",https://techcrunch.com/2017/08/10/microsoft-wants-to-make-blockchain-networks-enterprise-ready-with-its-new-coco-framework/
Anthony Ha,Enterprise,"Amplitude has raised $30 million to fund what CEO Spenser Skates said is a mission of “helping product people build better products.”
That’s not quite how Amplitude pitched itself initially, which was more about undercutting other analytics companies on price. Skates (pictured above with his co-founder Curtis Liu) said this is less a change in direction for the company than a new articulation of “what we’re good at.”
He added that Amplitude was “afraid to say that in the past,” partly because it was afraid of alienating marketers — a group that Skates is now comfortable insisting Amplitude is not built for. Instead, he said he’s focused on creating analytics tools for product teams.
What’s the difference? Skates said that when marketers look at analytics, they’re basically looking at where visitors are coming from and then how those visitors convert into paying customers: “Each step is a subset of the last step.” The product team, on the other hand, is trying to answer more difficult questions, like how different features affect long-term retention.

“Your users can do anything,” Skates said. “The questions you’re trying to answer are a lot more complex.”
Amplitude says that it has more than 5,000 customers, and in the past 12 months, it’s signed up big new ones including Microsoft, Capital One and Twitter.
In fact, in the funding announcement Microsoft’s Gooi Chungheong said, “Amplitude has saved us months of engineering investments into understanding user behavior in our products.”
The new Series C funding was led by IVP, with existing investors Benchmark Capital and Battery Ventures also participating. IVP’s Somesh Dash will become a board observer.
Amplitude has now raised a total of $59 million.",2017-08-10,1525390,https://tctechcrunch2011.files.wordpress.com/2017/08/amplitude-20.jpg?w=738,enterprise/,"analytics,ivp,amplitude",Amplitude raises another $30M for its product-focused analytics tools,,https://techcrunch.com/2017/08/10/amplitude-series-c/
Ron Miller,Enterprise,"When AWS today became a full-fledged member of the container standards body, the Cloud Native Computing Foundation, it represented a significant milestone. By joining Google, IBM, Microsoft, Red Hat and just about every company that matters in the space, AWS has acknowledged that when it comes to container management, standards matter.
AWS has been known to go the proprietary route, after all. When you’re that big and powerful, and control vast swaths of market share as AWS does, you can afford to go your own way from time to time. Containers is an area it hasn’t controlled, though. That belongs to Kubernetes, the open source container management tool originally developed inside Google.
AWS was smart enough to recognize that Kubernetes is becoming an industry standard in itself, and that when it comes to build versus buy versus going open source, AWS wisely recognized that battle has been fought and won.
Once it recognized Google’s dominance in container management, the next logical step was to join the CNCF and adhere to the same container standards the entire industry is using. Sometimes it’s better to switch than fight, and this was clearly one of those times.
What we have now is a clearer path to containerization, a technology that is all the rage inside large companies — for many good reasons. They allow you to break down the application into discrete manageable chunks, making updates a heck of a lot easier, and clearly dividing developer tasks and operations tasks in a DevOps model.
Standards provide a common basis for managing containers. Everyone can build their own tools on top of them. Google already has when it built Kubernetes, Red Hat has OpenShift, Microsoft makes Azure Container Service — and so forth and so on.
Companies like standards because they know the technology is going to work a certain way, regardless of who built it. Each vendor provides a similar set of basic services, then differentiates itself based on what it builds on top.
Technology tends to take off once a standard is agreed upon by the majority of the industry. Look at the World Wide Web. It has taken off because there is a standard way of building web sites. When companies agree to the building blocks, everything else seems to fall into place.
A lack of standards has traditionally held back technology. Having common building blocks just make sense. Sometimes a clear market leader doesn’t always agree. Today AWS showed why it matters, even to them.",2017-08-09,1525398,https://tctechcrunch2011.files.wordpress.com/2017/08/15670725648_03fae340ab_k1.jpg?w=738,enterprise/,"containers,standards,cncf,aws",AWS just proved why standards drive technology platforms,,https://techcrunch.com/2017/08/09/aws-just-proved-why-standards-drive-technology-platforms/
Ron Miller,Enterprise,"Red Hat issued its quarterly update to the OpenShift platform today, adding among other things, a Service Catalog that enables IT or third-party vendors to create connections to internal or external services.
OpenShift is RedHat’s Platform as a Service, based on Kubernetes, the open source container management platform, which was originally developed by Google. It also supports Docker, a popular container platform,  and adheres to the Open Container Initiative, a set of industry standards for containers, according to the company.
As companies make the shift from virtual machines to containers, there is an increasing need for platforms like OpenShift, and Red Hat is seeing massive interest from companies as varying as Deutsche Bank, Volvo and United Health.
OpenShift is a technology that’s helping these companies, and many others make that transition to containers, says Joe Fernandes, senior director of product management for OpenShift at Red Hat. “The adoption of container technology is off the charts. Pretty much every company is adopting containers in some way,” he said.
As the company increases its container customer base, it’s trying to build out the platform to meet the needs of larger company IT departments. One of the things they’ve been hearing is that they want it to be easier to connect containerized applications to internal and external services.
The Service Catalog is not unlike an app store in that developers can go in and find pre-configured connectors. This could be an internal connector to an Oracle database or an external one to a public cloud service from AWS or Azure (or anything else). Fernandes says the app store analogy is apt, but points out that it doesn’t have any procurement capability, at least for now. It could in the future, he says.
Customers were able to make these service connections before, but it took a lot more effort. The goal is to provide a packaged approach so that users don’t have to do the work every time, which should help speed up implementation.
The Service Catalog is available as a technical preview for this release. The next release is due before the end of the year.",2017-08-09,1524854,https://tctechcrunch2011.files.wordpress.com/2016/06/gettyimages-546996765.jpg?w=738,enterprise/,"open-source,docker,kubernetes,red-hat",Red Hat updates OpenShift container platform with new service catalog,,https://techcrunch.com/2017/08/09/red-hat-updates-openshift-platform-with-new-service-catalog/
Frederic Lardinois,Enterprise,"Business intelligence and analytics firm Tableau today announced that it has acquired ClearGraph, a service that lets you query and visualize large amounts of business date through natural language queries (think “this week’s transactions over $500”). Tableau expects to integrate this technology with its own products as it looks to make it easier for its users to use similar queries to visualize their data.
Typically, you’d have to know SQL or a similar database query language to pull information out of most enterprise databases. Recent advances in natural language processing and machine learning now allow services like ClearGraph to understand more about the underlying database and then take these sentences and essentially translate them into database queries. Given that Microsoft’s Power BI and other competitors already offer this capability, it’s no surprise that Tableau is also looking into this (though Tableau argues that — unlike the likes of Microsoft — it can be a neutral party given that it has no investment in any particular cloud or on-premise technology outside of its own).
Indeed, Tableau’s Chief Product Officer Francois Ajenstat tells me that the company actually started an internal project to build a conversational interface for its service. To build this, though, Tableau would have had to put a lot of infrastructure in place and ClearGraph had already done all of this work.
ClearGraph was founded in 2014 (and it was previously called Argo and Arktos) and the company says that it currently has “dozens” of customers, including a number of large enterprises. While the company doesn’t disclose who its customers are (which is not unusual in the enterprise space), Ajenstat tells me that they include financial institutions, retailers and major internet companies. As Ajenstat noted, every company today struggles to make its data accessible to more of its employees, so it doesn’t come as a surprise that ClearGraph’s customers span a wide range of verticals.

Existing ClearGraph customers won’t notice any immediate changes, but, over time, once it has integrated this new technology, Tableau will likely transition them to its own platform.
Ajenstat also believes that this new technology will help his company reach a wider range of users. “Even though Tableau is top in its class for ease of use, it’s about expanding the number of users who can analyze data in the enterprise,” he told me. He also noted that while this next group of potential users may be casual users, their questions aren’t simple — and that’s where Tableau thinks ClearGraph’s natural language processing technology will help it succeed.
The ClearGraph team will join Tableau’s Palo Alto office and focus on integrating its technology with Tableau’s.
The company declined to disclose the purchase price, but ClearGraph raised a total of $1.53 million from Accel Partners before the acquisition.",2017-08-09,1524671,https://tctechcrunch2011.files.wordpress.com/2017/08/1__tableau_software_b-roll_overview_201304_-_youtube.png?w=738,enterprise/,"nlp,cleargraph,tableau","Tableau acquires ClearGraph, a startup that lets you analyze your data using natural language",,https://techcrunch.com/2017/08/09/tableau-acquires-cleargraph-a-startup-that-lets-you-analyze-your-data-using-natural-language/
Ron Miller,Cloud,"BlueJeans Network, the cloud video and collaboration company, announced today that Quentin Gallivan, an industry veteran who has helped run several tech companies, will be taking over as CEO.
Former CEO and company founder Krish Ramakrishnan will remain with the company and take on the role of executive chairman. He will also continue to lead strategy and innovation.
Ramakrishnan doesn’t see this move as stepping back. He said that he actually pursued Gallivan precisely because he has a track record of scaling companies like Pentaho, Verisign and Postini –and he believed he needed a leader that could take the company to the next level. Meanwhile, Ramakrishnan will be freed up to concentrate on other projects that could take the company’s core video conferencing and collaboration business in new directions.
As Gallivan comes in the door, he takes over a company that he sees in pretty good shape — one of the primary reasons he was attracted to the job — but that requires some additional tweaking to go public eventually and all that entails. That means becoming a company fully committed to the needs of large enterprise customers.
The two leaders developed a bromance when they met informally at a gathering about 9 months ago. Ramakrishnan was soon convinced that Gallivan was the perfect fit to take over the CEO duties at BlueJeans. First, he had to talk Gallivan into leaving his position as CEO at Pentaho, the data and analytics company he had been running for over five years.
As the two talked, Gallivan discovered that Ramakrishnan was looking to take on a different role. Gallivan liked what he saw, a well capitalized company poised for growth. While he hadn’t worked with video in his previous positions, he said as he explored the company, he learned that it was playing an increasingly important role in enterprise communication and collaboration.
Of course he’s not the only one to notice that. Big players like Cisco Google and Microsoft have taken a shine to video communications. Facebook has too, although BlueJeans says they’re a partner with both Facebook Live and Workplace by Facebook.
Other video startups are also extremely well capitalized. Zoom raised $100 million in January on a billion dollar valuation. Another competitor, Fuze has raised $330M with $104 million coming in one big chunk in February (when it also hired a new CEO).
In spite of the heavy competition, Gallivan believes that BlueJeans is well positioned to take on all comers. “It’s taking the foundation we have and building off of that and completely focusing on the enterprise space,” he explained. He believes that the enterprise offers the greatest potential revenue and the best path to differentiating itself from the list of worthy competitors in this space.
BlueJeans has been around since 2009 and has raised over $175 million. Investors include Accel Partners, NEA, Battery Ventures and Derek Jeter’s Jeter Ventures.",2017-08-08,1524404,https://tctechcrunch2011.files.wordpress.com/2017/08/bjn-photo.jpg?w=738,enterprise/,"collaboration,video",BlueJeans Network names industry vet Quentin Gallivan as CEO,,https://techcrunch.com/2017/08/08/industry-vet-quentin-gallivan-takes-over-as-ceo-at-bluejeans-network/
Ron Miller,Artificial Intelligence,"Brands have long been able to search for company mentions on social media, but they’ve lacked the ability to search for pictures of their logos or products in an easy way. That’s where Salesforce’s latest Einstein artificial intelligence feature comes into play.
Today the company introduced Einstein Vision for Social Studio, which provides a way for marketers to search for pictures related to their brands on social media in the same way they search for other mentions. The product takes advantage of a couple of Einstein artificial intelligence algorithms including Einstein Image Classification for image recognition. It uses visual search, brand detection and product identification. It also makes use of Einstein Object Detection to recognize objects within images including the type and quantity of object.
AI has gotten quite good at perception and cognition tasks in recent years. One result of this has been the ability to train an algorithm to recognize a picture. With cheap compute power widely available and loads of pictures being uploaded online, it provides a perfect technology combination for better image recognition.
Rob Begg, VP of product marketing for social and advertising products at Salesforce, says it’s about letting the machine loose on tasks for which it’s better suited. “If you think of it from a company point of view, there is a huge volume of tweets and [social] posts. What AI does best is help surface and source the ones that are relevant,” he says.
As an example, he says there could be thousands of posts about cars, but only a handful of those would be relevant to your campaign. AI can help find those much more easily.
Begg sees three possible use cases for this tool. First of all, it could provide better insight into how people are using your products. Secondly it could provide a way to track brand displays online hidden within pictures, and finally it could let you find out when influencers such as actors or athletes are using your products.
The product comes trained to recognize two million logos, 60 scenes (such as an airport), 200 foods and 1000 objects. That should be enough to get many companies started. Customizing isn’t available in the first release, so if you have a logo or object not included out of the box, you will need to wait for a later version to be able to customize the content.
Begg says it should be fairly easy for marketers used to using Social Studio to figure out how to incorporate the visual recognition tools into their repertoire. The new functionality should be available immediately to Salesforce Social Studio users.",2017-08-08,1524115,https://tctechcrunch2011.files.wordpress.com/2017/08/gettyimages-112807391.jpg?w=738,enterprise/,"social-media-marketing,image-recognition",Salesforce AI helps brands track images on social media,salesforce,https://techcrunch.com/2017/08/08/salesforce-using-ai-to-find-brand-images-on-social-media/
John Mannes,Artificial Intelligence,"Two months ago, Facebook’s AI Research Lab (FAIR) published some impressive training times for massively distributed visual recognition models. Today IBM is firing back with some numbers of its own. IBM’s research groups says it was able to train ResNet-50 for 1k classes in 50 minutes across 256 GPUs — which is effectively just the polite way of saying “my model trains faster than your model.” Facebook noted that with Caffe2 it was able to train a similar ResNet-50 model in one hour on 256 GPUs using an 8k mini-batch approach.
This would be a natural moment to question why any of this matters in the first place. Distributed processing is a big sub-field of AI research, but it’s also quite arcane. Computing jobs are often so big for deep learning problems that they are most efficiently handled across a large number of GPUs instead of just a single GPU.
But as you add more GPUs, training time doesn’t naturally scale down. For example, you might assume that if it took two minutes to train with one GPU it would take one minute to train with two GPUs. In the real world it doesn’t work like this because there is some cost to splitting up and recombining complex quantitative operations.
What IBM is promising is the most efficient distributed deep learning library for breaking up a giant deep learning problem into hundreds of smaller deep learning problems. This all might seem petty in the context of a single compute job, but remember that companies like IBM and Facebook are training models all day, every day for millions of customers. Every major tech company has a stake in this, but it’s often tough to compare results companies promise because of the sheer number of variables in any research effort.
Now you would be right to question the future meaningfulness of obsessing on incremental increases in distributed scaling efficiency — and you’d be right. Hillery Hunter, director of systems acceleration and memory at IBM Research, tells me that everyone is getting really close to optimal.
“You have gotten about as much as you can out of the system and so we believe we are close to optimal. The question is really the rate at which we keep seeing improvements and whether we are still going to see improvements in the overall learning times.”
IBM didn’t stop with just the ResNet-50 results. The company continued the work testing distributed training on ResNet-101, a much larger and more complex visual recognition model. The team says that it was able to train ResNet-101 on the ImageNet-22k data set with 256 GPUs in seven hours, a fairly impressive time for the challenge.
“This also benefits folks running on smaller systems,” Hunter added.”You don’t need 256 GPUs and 64 systems to get the benefits.”
The deep learning library plays well with the major open-source deep learning frameworks, including TensorFlow, Caffe and Torch. Everything will be available via PowerAI if you want to try things out for yourself.",2017-08-07,1524487,https://tctechcrunch2011.files.wordpress.com/2017/08/gettyimages-166272733.jpg?w=738,enterprise/,"ai,ibm,computing,gpgpu",IBM touts improved distributed training time for visual recognition models,,https://techcrunch.com/2017/08/07/ibm-touts-improved-distributed-training-time-for-visual-recognition-models/
John Mannes,Artificial Intelligence,"Facebook announced this morning that it had completed its move to neural machine translation — a complicated way of saying that Facebook is now using convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to automatically translate content across Facebook.
Google, Microsoft and Facebook have been making the move to neural machine translation for some time now, rapidly leaving old-school phrase-based statistical machine translation behind. There are a lot of reasons why neural approaches show more promise than phrase-based approaches, but the bottom line is that they produce more accurate translations.
Traditional machine translation is a fairly explicit process. Relying on key phrases, phrase-based systems translate sentences then probabilistically determine a final translation. You can think of this in a similar light as using the Rosetta Stone (identical phrases in multiple languages) to translate text.
In contrast, neural models deal in a higher level of abstraction. The interpretation of a sentence becomes part of a multi-dimensional vector representation, which really just means we’re trying to translate based on some semblance of “context” rather than phrases.
It’s not a perfect process, and researchers are still tinkering with how to deal with long-term dependencies (i.e. retaining understanding and accuracy throughout a long text), but the approach is incredibly promising and has produced great results, thus far, for those implementing it.
Google announced the first stage of its move to neural machine translation in September 2016 and Microsoft made a similar announcement two months later. Facebook has been working on its conversion efforts for about a year and it’s now at full deployment. Facebook AI Research (FAIR) published its own research on the topic back in May and open sourced its CNN models on GitHub.
“Our problem is different than that of most of the standard places, mostly because of the type of language we see at Facebook,” Necip Fazil Ayan, engineering manager in Facebook’s language technologies group, explained to me in an interview. “We see a lot of informal language and slang acronyms. The style of language is very different.”
Facebook has seen about a 10 percent jump in translation quality. You can read more into the improvement in FAIR’s research. The results are particularly striking for languages that lack a lot of data in the form of comparative translation pairs.",2017-08-03,1523143,https://tctechcrunch2011.files.wordpress.com/2014/12/shutterstock_75513031.jpg?w=738,enterprise/,"applied-mathematics,artificial-intelligence,artificial-neural-networks",Facebook finishes its move to neural machine translation,facebook,https://techcrunch.com/2017/08/03/facebook-finishes-its-move-to-neural-machine-translation/
Ingrid Lunden,Enterprise,"Kabbage, a company with some 115,000 customers and $3.5 billion in loans that has built an automated platform for lending money to small businesses and individuals using a large set of data points to determine a customer’s credit score, is announcing some big cabbage of its own today.
SoftBank Group is investing $250 million in Kabbage — funding that Rob Frohwein, the co-founder and CEO, said the company plans to use to expand its business in the U.S.; launch yet more analytics tools to provide loans for specific verticals; expand into new markets like Asia; and to explore acquisitions to add new products to its business, like payments.
The investment, a Series F, brings the total raised by Kabbage in equity funding to $500 million (on top of some $3.5 billion in securities), and will give a bump to the company’s billion-dollar-plus valuation.
For some context: In 2015, when it raised a Series E of $135 million, Kabbage entered the so-called echelon of “unicorn” startups with a valuation of $1 billion. In an interview with TechCrunch this week, CEO and co-founder Rob Frohwein declined to name a specific number but said that this latest investment was at a “meaningful upround” that was more than $1.25 billion but not quite $2 billion.
Founded in Atlanta in 2009, Kabbage was an early mover in the concept of using big data analytics to underwrite and monitor loans: the company draws on hundreds of sources of information, from a company’s or individual’s public social media profiles and a business’s QuickBooks accounts, through to larger macro data sets, to decide whether or not to loan money (and how much to loan).
At a time when a number of other online small business lenders like OnDeck and Can Capital have stumbled under iffy business models, amid a wider shakedown in the online loan industry overall, Kabbage has grown. Frohwein attributes this squarely to its big data play, which collectively now crunches some 1.5 million data connections to help make decisions.
“When the proverbial shit hit the fan in the online lending space a little while back, people at Kabbage were nervous. All I said was, ‘finally!'” Frohwein recalled. “What’s happened is that now we have some separation between companies that were not focused on building differentiated solutions and those that are trying to do something different. We’ve gotten past the point of puffery. A small handful of companies have differentiated themselves and I think Kabbage is one of those.”
He said that Kabbage is profitable in its loans business, but not yet in its platform operation — the latter is a newer division launched in 2015 that essentially powers other lending businesses, alongside Kabbage’s own retail operation. (Customers include Kabbage itself, which has a Karrot consumer loans business; as well as major banks like ING, Santander and Scotiabank.) “As a company, we will be profitable in Q4 from a GAAP perspective,” he said.
While Kabbage’s big data formula is today used by several other fintech companies — they include Kreditech (which focuses on helping to create credit scores for people who are “unbanked” and is backed by Peter Thiel and Naspers), Fundbox (also with a long list of interesting investors including Jeff Bezos) and BlueVine (backed by Citi Group, among others) — Kabbage plans to take its own data play to the next level.
One area will be loans for specific verticals or types of businesses: the idea here is that if you are a construction business or a restaurant, you are likely to have very different cash flows, so Kabbage would like to develop ways of making the loans more personalised and less full of default and friction that can easily be avoided if the lender understands more about the borrower.
Today, Kabbage already claims stronger customer loyalty than many other lenders: Frohwein said that on average its customers borrow 20 times from Kabbage over three to four years: he claimed that as a point of comparison the industry average is 2.2 times.
We should also keep an eye out for acquisitions and completely new product launches from the company. Some have suggested it might try to buy OnDeck in a consolidation play, but our sources say this was just speculation and that a more likely scenario would be acquiring companies to add new services to the platform, rather than as a consolidation play among similar competitors.
Asked about this, Frohwein would not specify what new product launches or acquisitions might come first, but he highlighted payments as an interesting area.
“Look at how PayPal and square have both gone from being payments companies to small business lenders,” he noted. “There is a clear connection between how companies think about those two business areas, so it’s not a giant leap of faith for us to consider this.”
While SoftBank has been making some major waves in the investment world lately with huge investments out of its new $100 billion Vision Fund, this latest effort with Kabbage comes from the SoftBank Group directly (and somehow $250 million sounds suddenly modest when you are thinking about $100 billion). From what I understand, it could potentially get rolled into the Vision Fund later down the line, but for now it represents some very interesting strategic opportunities for Kabbage.
Those could come in two general forms. First, there is expansion of Kabbage’s retail and platform businesses further into Asia (where it’s only so far worked as a white-label platform partner).
Second, there is the opportunity to work with a myriad of SoftBank’s portfolio companies. Nothing announced so far on this front, but recall that SoftBank’s diverse holdings include Sprint (which has a large number of small business customers); Lyft and a number of other ride-sharing startups (another business line that relies heavily on very small businesses: sole traders); SoFi; and many more.
In that regard, SoftBank’s investment looks like a smart bet placed on a company that it believes it could leverage throughout its network in some interesting ways.
“SoftBank invests in market-leading companies that dramatically improve the customer experience and expand markets through breakthrough technology and data capabilities,” said SoftBank Managing Director David Thevenon, in a statement. “We invested in Kabbage because their unique automated lending platform leverages open data networks and best positions them to empower small businesses around the world.”",2017-08-03,1522907,https://tctechcrunch2011.files.wordpress.com/2014/04/5110067974_95095620fa_b.jpg?w=738,enterprise/,"loans,small-business,smbs,kabbage",Small business loan platform Kabbage nabs $250M from Softbank,,https://techcrunch.com/2017/08/03/small-business-loan-platform-kabbage-nabs-250m-from-softbank/
Frederic Lardinois,Enterprise,"When you first look at Redkix, it looks like any other Slack clone, but while you could definitely use it just like Slack, the team offers an important twist on the standard company chat theme: it plays nice with email. After a year of private testing with about 7,000 users, the team is opening up its public beta today and launching its paid premium program in private beta.
Oudi Antebi, Redkix’s CEO and co-founder, told me that one of his main theses for building Redkix was that collaboration software only works when you have 100 percent participation inside a company. “All you need is one person to ruin the party for everyone else,” he said. In addition, users don’t want to have to switch back and forth between their inbox and their chat tools.
To solve this, the team decided to make email a core feature of Redkix. That means the service features both an email inbox for your work email and you can use the regular chat window to talk to users — maybe outside of your company — who only use email. Redkix simply aggregates your messages to them and sends them out as an email and when they respond, their answers show up in a chat channel, too. The overall experience is pretty intuitive and straightforward, though there’s obviously a tiny bit of lag before the service parses incoming emails.

One feature Antebi is especially proud of is threads on Redkix. If you’re a regular Slack users, then you know that this remains an unsolved problem in chat apps (and Slack’s solution still feels clunky to me). Redkix threads live inside their channels but feel more like separate entities that are easier to keep track of than their Slack counterparts.
Antebi also stressed that Redkix will soon be ISO 27001 certified and that the company plans to get Soc 2, Soc 3 and HIPAA certifications, too. This clearly speaks to the company’s ambition in the enterprise space, where having these kind of enterprise-level security and privacy certifications are table stakes.
Looking ahead, we’ll like see support for video and audio chats as well, though it’s unclear when exactly this will happen.
The new premium “Teams” tier, which will cost $6 per user and month, is now also available in private beta. The playbook here also sounds quite similar to Slack’s. Users pay for having unlimited team members (the free version is restricted to 10) and an unlimited chat and email histories, as well as a larger file storage allotment (10GB). Later on, the team also plans to launch an enterprise version with support for on-premise content storage, advanced encryption and support for Active Directory sync.
Native apps for the service are currently available on Windows and MacOS, as well as on the iPhone. The Android app is still in private beta for now and a web-based version of Redkix is launching soon.
With about $20 million in funding to date, plans to grow the team to 60 people by the end of the year and offices in Israel and the U.S., Redkix is clearly planning for growth. How well the company will do in what is an increasingly saturated market for collaboration tools remains to be seen, but its slick email integration may just put it over the top when it goes out to sell its services to enterprises.",2017-08-02,1522135,https://tctechcrunch2011.files.wordpress.com/2017/08/gettyimages-479977831.jpg?w=738,enterprise/,"collaboration,redkix","Redkix, an email-friendly team messaging platform, launches its public beta",slack,https://techcrunch.com/2017/08/02/redkix-an-email-friendly-team-messaging-platform-launches-its-public-beta/
Darrell Etherington,Enterprise,"HP has a new entrant in that most curious PC niche – the backpack computer. A product of the virtual reality computing wave, the backpack PC provides all the power needed to drive high-quality VR headsets like Oculus Rift and HTC Vive, but with a form factor that allows the user to roam about untethered. The new HP Z VR Backpack is a bit different from the rest of the field, though, since it’s designed specifically as a workstation.
The HP Z contains an Nvidia Quadro P5200 GPU, and it’s the first time the mobile workstation-class graphics card has been used in a VR backpack to date. The whole outfit weighs in at 10 lbs, and the GPU’s capabilities allow it to run complex simulations and of course power top-end VR experiences, all delivered with cordless performance thanks to a built-in battery.

HP is looking to start shipping the Z VR in September, with a starting price of $3,299 and additional options available for additional cost. It’s also getting a companion dock that allows you to use it more like a traditional desktop for those times when you don’t need to be roaming around immersed in virtual environments.

This whole thing might sound like a bit of a boondoggle, but one of the most interesting potential use cases for VR is actually in engineering and design, where the ability to prototype virtually and make changes prior to creating full-scale physical models can save a lot of time and effort. Companies like Lucid, which is hoping to bring a Tesla Model S competitor to market, are already demonstrating how VR can help cut down on pre-production expense.
The Z VR would also be useful for job training, HP suggests, and for fields like telemedicine and even commercial VR experience centers. It’s probably still going to be quite niche, but HP is clearly hoping that niche expands over time as VR becomes more prevalent across industries.",2017-08-02,1522456,https://tctechcrunch2011.files.wordpress.com/2017/08/hp_z_vr_backpack_used_by_product_designers.jpg?w=738,enterprise/,"nvidia,hp","HP’s new Nvidia-powered backpack VR PC is designed for work, not play",,https://techcrunch.com/2017/08/02/hps-new-nvidia-powered-backpack-vr-pc-is-designed-for-work-not-play/
Ron Miller,Cloud,"Oracle might not be the first company you think of when it comes to cloud computing, but the company has made significant strides in recent years. Today, it announced the bi-annual update to its Oracle Cloud Applications Suite.
This is update number 13 for those keeping score at home. The suite includes a range of enterprise software including ERP (think back-office management), HR and CRM/CX (for customer management). While there were a number of changes across the individual pieces, the entire suite got a fresh design.
The latest version certainly has a more modern look and feel, and that was the idea, says Steve Miranda, executive vice president of applications development at Oracle. Miranda says it’s a brand new experience compared to the previous version of the software. The company not only overhauled the design, it also wanted to improve workflows, working to take out unnecessary steps when possible.
What’s more, Oracle has turned to a responsive design approach, where the interface adjusts depending on screen size. It’s a marked improvement over the previous mobile experience, Miranda says, which used different colors and visualizations and a different navigation paradigm. He says previously they had some apps designed specifically for mobile, but some simply were presented in the browser, and didn’t always translate well to a smaller screen. Today’s update is designed to address that.
 
As for the ERP piece, the company enhanced the supply chain software that enables companies to track supplies in a manufacturing process. In the CX customer cloud, the company is adding brand new functionality that has been designed to improve communication between sales and marketing, a challenge that seems to face just about every organization, and one which vendors are forever working to resolve.
As Oracle more fully embraces the cloud, it’s adding new functionality and expanding the product set whenever possible, and this update appears to reflect that. It’s worth noting that Miranda says the company still maintains an on-prem version of the suite, but the cloud version with the twice yearly updates is now the more popular. On-prem customers will have to wait to see these changes, as it tends to be on a much slower update cadence than the cloud.",2017-08-02,1522124,https://tctechcrunch2011.files.wordpress.com/2017/08/gettyimages-692915429.jpg?w=738,enterprise/,"crm,erp,oracle-cloud",Oracle delivers bevy of updates to its cloud suite,oracle,https://techcrunch.com/2017/08/02/oracle-makes-bevy-of-updates-to-its-cloud-suite/
Ron Miller,Cloud,"ProoV, a startup founded by a couple of industry veterans, solves a problem the founders encountered many times over the years: how do you get the CIO or CTO to agree to do a proof of concept for your startup to show that your idea will actually work as you’ve described.
Today the company announced a $14 million Series B financing round led by Helios Capital and Mangrove Capital Partners with participation from OurCrowd and Cerca Partners. The latest investment brings the total raised to $21.1 million to date, and is part of an 18 month stretch in which the company went from seed to Series A to Series B.
The idea behind ProoV is to provide a place for companies to conduct proofs of concept with startups quickly and efficiently. Most larger companies want to find ways to use more modern tools, but they are often stuck for a variety of reasons including lack of internal resources to run a PoC, regulatory hurdles around using customer data for testing purposes and skepticism that the startup can deliver what it’s promising. ProoV is supposed to help alleviate all of that by offering a platform of services for building and executing a proof of concept project.
The company has built a platform that is part app store and part PoC service engine. Enterprise customers can connect with any of the 1000 approved startups and conduct proofs of concept using data ProoV has built, which is itself based on the way the customer structures its data sets. This saves the customer from using live customer data and should settle any regulatory issues.
So far, the company has 125 enterprise customers including big names like GE, Amazon Web Services and AIG using the platform.
ProoV makes money in a number of ways including charging a fee for the startups to be included on the platform. The majority of the revenue, however, comes from enterprise customers, who pay a subscription fee for access to the platform (and the startups listed there) and then a per proof of concept price. The company is experimenting with different ways of mixing and matching subscription and usage pricing.
For now, the plan is to use this round’s money to set up a new office in New York City and hire 15 salespeople to try to drive growth. The company currently has 37 employees. The R&D arm is based in Israel.",2017-08-02,1521905,https://tctechcrunch2011.files.wordpress.com/2017/08/gettyimages-638236798.png?w=738,enterprise/,"helios-capital,proof-of-concept,proov",ProoV scores $14 million Series B for proof of concept platform,,https://techcrunch.com/2017/08/02/proov-scores-14-million-series-b-for-proof-of-concept-platform/
